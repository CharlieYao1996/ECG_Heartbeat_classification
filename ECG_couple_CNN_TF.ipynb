{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models, Input  \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. load data & Dataset define\n",
    "# =========================\n",
    "data_dir = r\".\\archive\"\n",
    "train_path = os.path.join(data_dir, \"mitbih_train.csv\")\n",
    "test_path = os.path.join(data_dir, \"mitbih_test.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path, header=None)\n",
    "test_df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "X_train = train_df.iloc[:, :-1].values\n",
    "y_train = train_df.iloc[:, -1].values.astype(int)\n",
    "\n",
    "X_test = test_df.iloc[:, :-1].values\n",
    "y_test = test_df.iloc[:, -1].values.astype(int)\n",
    "\n",
    "# Z-score per sample\n",
    "X_train = (X_train - X_train.mean(axis=1, keepdims=True)) / (X_train.std(axis=1, keepdims=True) + 1e-8)\n",
    "X_test = (X_test - X_test.mean(axis=1, keepdims=True)) / (X_test.std(axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "# reshape: (samples, 187, 1), tf dif\n",
    "X_train = X_train[:, :, np.newaxis]\n",
    "X_test = X_test[:, :, np.newaxis]\n",
    "\n",
    "\n",
    "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "\n",
    "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test_tf = tf.convert_to_tensor(y_test, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defind model, couple CNN,\n",
    "def ECG_CNN_TF(input_shape=(187, 1), num_classes=5):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "    \n",
    "    # Conv Block 1\n",
    "    model.add(layers.Conv1D(64, kernel_size=5, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv1D(64, kernel_size=5, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Conv Block 2\n",
    "    model.add(layers.Conv1D(128, kernel_size=5, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    \n",
    "    model.add(layers.Conv1D(128, kernel_size=5, padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.MaxPooling1D(pool_size=2, strides=2))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # FC layers\n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Output\n",
    "    model.add(layers.Dense(num_classes))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "save_dir = os.path.join(data_dir, \"models\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_path = os.path.join(save_dir, \"ECG_couple_TF.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 64ms/step - accuracy: 0.9395 - loss: 0.2143 - val_accuracy: 0.9709 - val_loss: 0.1052\n",
      "Epoch 2/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 71ms/step - accuracy: 0.9677 - loss: 0.1148 - val_accuracy: 0.9702 - val_loss: 0.0980\n",
      "Epoch 3/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9742 - loss: 0.0907 - val_accuracy: 0.9788 - val_loss: 0.0753\n",
      "Epoch 4/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 67ms/step - accuracy: 0.9776 - loss: 0.0774 - val_accuracy: 0.9795 - val_loss: 0.0790\n",
      "Epoch 5/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9804 - loss: 0.0689 - val_accuracy: 0.9788 - val_loss: 0.0820\n",
      "Epoch 6/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9820 - loss: 0.0617 - val_accuracy: 0.9832 - val_loss: 0.0620\n",
      "Epoch 7/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 74ms/step - accuracy: 0.9828 - loss: 0.0584 - val_accuracy: 0.9805 - val_loss: 0.0670\n",
      "Epoch 8/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 71ms/step - accuracy: 0.9844 - loss: 0.0522 - val_accuracy: 0.9839 - val_loss: 0.0571\n",
      "Epoch 9/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 74ms/step - accuracy: 0.9858 - loss: 0.0476 - val_accuracy: 0.9841 - val_loss: 0.0573\n",
      "Epoch 10/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 70ms/step - accuracy: 0.9862 - loss: 0.0455 - val_accuracy: 0.9838 - val_loss: 0.0578\n",
      "Epoch 11/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 70ms/step - accuracy: 0.9869 - loss: 0.0430 - val_accuracy: 0.9858 - val_loss: 0.0521\n",
      "Epoch 12/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9880 - loss: 0.0393 - val_accuracy: 0.9846 - val_loss: 0.0548\n",
      "Epoch 13/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9884 - loss: 0.0369 - val_accuracy: 0.9852 - val_loss: 0.0528\n",
      "Epoch 14/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9882 - loss: 0.0350 - val_accuracy: 0.9859 - val_loss: 0.0506\n",
      "Epoch 15/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9898 - loss: 0.0331 - val_accuracy: 0.9856 - val_loss: 0.0536\n",
      "Epoch 16/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 71ms/step - accuracy: 0.9902 - loss: 0.0305 - val_accuracy: 0.9857 - val_loss: 0.0517\n",
      "Epoch 17/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 69ms/step - accuracy: 0.9900 - loss: 0.0310 - val_accuracy: 0.9855 - val_loss: 0.0555\n",
      "Epoch 18/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 70ms/step - accuracy: 0.9912 - loss: 0.0280 - val_accuracy: 0.9870 - val_loss: 0.0546\n",
      "Epoch 19/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 68ms/step - accuracy: 0.9913 - loss: 0.0263 - val_accuracy: 0.9865 - val_loss: 0.0516\n",
      "Epoch 20/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 70ms/step - accuracy: 0.9914 - loss: 0.0259 - val_accuracy: 0.9861 - val_loss: 0.0533\n",
      "Epoch 21/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 70ms/step - accuracy: 0.9921 - loss: 0.0246 - val_accuracy: 0.9863 - val_loss: 0.0525\n",
      "Epoch 22/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 70ms/step - accuracy: 0.9920 - loss: 0.0239 - val_accuracy: 0.9865 - val_loss: 0.0520\n",
      "Epoch 23/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 73ms/step - accuracy: 0.9924 - loss: 0.0231 - val_accuracy: 0.9864 - val_loss: 0.0510\n",
      "Epoch 24/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 73ms/step - accuracy: 0.9926 - loss: 0.0222 - val_accuracy: 0.9868 - val_loss: 0.0494\n",
      "Epoch 25/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9931 - loss: 0.0217 - val_accuracy: 0.9868 - val_loss: 0.0507\n",
      "Epoch 26/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9937 - loss: 0.0196 - val_accuracy: 0.9857 - val_loss: 0.0566\n",
      "Epoch 27/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9930 - loss: 0.0215 - val_accuracy: 0.9873 - val_loss: 0.0518\n",
      "Epoch 28/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 73ms/step - accuracy: 0.9935 - loss: 0.0190 - val_accuracy: 0.9876 - val_loss: 0.0519\n",
      "Epoch 29/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 72ms/step - accuracy: 0.9937 - loss: 0.0201 - val_accuracy: 0.9864 - val_loss: 0.0553\n",
      "Epoch 30/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 73ms/step - accuracy: 0.9938 - loss: 0.0183 - val_accuracy: 0.9865 - val_loss: 0.0532\n",
      "Epoch 31/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 74ms/step - accuracy: 0.9941 - loss: 0.0181 - val_accuracy: 0.9871 - val_loss: 0.0520\n",
      "Epoch 32/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 73ms/step - accuracy: 0.9941 - loss: 0.0176 - val_accuracy: 0.9863 - val_loss: 0.0521\n",
      "Epoch 33/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 75ms/step - accuracy: 0.9939 - loss: 0.0178 - val_accuracy: 0.9873 - val_loss: 0.0532\n",
      "Epoch 34/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 76ms/step - accuracy: 0.9944 - loss: 0.0169 - val_accuracy: 0.9871 - val_loss: 0.0557\n",
      "Epoch 35/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 78ms/step - accuracy: 0.9948 - loss: 0.0170 - val_accuracy: 0.9865 - val_loss: 0.0557\n",
      "Epoch 36/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 73ms/step - accuracy: 0.9949 - loss: 0.0152 - val_accuracy: 0.9856 - val_loss: 0.0569\n",
      "Epoch 37/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 74ms/step - accuracy: 0.9951 - loss: 0.0149 - val_accuracy: 0.9865 - val_loss: 0.0549\n",
      "Epoch 38/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 74ms/step - accuracy: 0.9949 - loss: 0.0159 - val_accuracy: 0.9869 - val_loss: 0.0579\n",
      "Epoch 39/100\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 74ms/step - accuracy: 0.9949 - loss: 0.0154 - val_accuracy: 0.9863 - val_loss: 0.0531\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3.Training\n",
    "# =========================\n",
    "\n",
    "#keep same seed for all model\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "#Stratified split\n",
    "class_ranges = [\n",
    "    (0, 72471),\n",
    "    (72471, 74694),\n",
    "    (74694, 80483),\n",
    "    (80483, 81123),\n",
    "    (81123, 87554)\n",
    "]\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for start, end in class_ranges:\n",
    "    idx = np.arange(start, end)\n",
    "    np.random.shuffle(idx)\n",
    "    n_val = int(len(idx) * 0.2)\n",
    "    val_indices.extend(idx[:n_val])\n",
    "    train_indices.extend(idx[n_val:])\n",
    "\n",
    "X_train_split = X_train[train_indices]\n",
    "y_train_split = y_train[train_indices]\n",
    "X_val_split = X_train[val_indices]\n",
    "y_val_split = y_train[val_indices]\n",
    "\n",
    "#Dataset + batch\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_split, y_train_split))\n",
    "train_dataset = train_dataset.shuffle(len(X_train_split)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val_split, y_val_split))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test_tf, y_test_tf))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "model = ECG_CNN_TF(input_shape=(187,1), num_classes=5)\n",
    "# Loss function \n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Optimizer AdamW\n",
    "optimizer = tf.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-3)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "save_path = model_path\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(save_path, monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step\n",
      "Evaluation CSV saved at .\\archive\\models\\test_pred.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# evaluation\n",
    "# =========================\n",
    "batch_size = 128\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# load model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "preds_prob = model.predict(test_dataset)  # shape: (num_samples, num_classes)\n",
    "#predict \n",
    "preds = np.argmax(preds_prob, axis=1)     \n",
    "#save CSV\n",
    "csv_path = os.path.join(save_dir, \"test_pred.csv\")\n",
    "pd.DataFrame({\"y_true\": y_test, \"y_pred\": preds}).to_csv(csv_path, index=False)\n",
    "print(f\"Evaluation CSV saved at {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Confusion Matrix =====\n",
      "[[18047    49    18     2     2]\n",
      " [   91   454    11     0     0]\n",
      " [   44     3  1383    18     0]\n",
      " [   22     0    13   127     0]\n",
      " [   16     0     1     0  1591]]\n",
      "\n",
      "===== Per-Class Metrics =====\n",
      "recall: [0.9961 0.8165 0.9551 0.784  0.9894]\n",
      "specificity: [0.9542 0.9976 0.9979 0.9991 0.9999]\n",
      "precision: [0.9905 0.8972 0.9698 0.8639 0.9987]\n",
      "f1: [0.9933 0.855  0.9624 0.822  0.9941]\n",
      "\n",
      "===== Macro-Average Metrics =====\n",
      "recall: 0.9082\n",
      "specificity: 0.9897\n",
      "precision: 0.9441\n",
      "f1: 0.9254\n",
      "\n",
      "===== Weighted-Average Metrics =====\n",
      "recall: 0.9868\n",
      "specificity: 0.9618\n",
      "precision: 0.9864\n",
      "f1: 0.9865\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# confusion matrix & metrics\n",
    "# =========================\n",
    "cm = confusion_matrix(y_test, preds, labels=list(range(5)))\n",
    "\n",
    "print(\"===== Confusion Matrix =====\")\n",
    "print(cm)\n",
    "\n",
    "# calculate metrics for 5 classes\n",
    "metrics_per_class = {\"recall\": [], \"specificity\": [], \"precision\": [], \"f1\": []}\n",
    "class_counts = cm.sum(axis=1)\n",
    "total_samples = class_counts.sum()\n",
    "weights = class_counts / total_samples\n",
    "\n",
    "for i in range(5):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "    recall_i = TP / (TP + FN + 1e-8)\n",
    "    specificity_i = TN / (TN + FP + 1e-8)\n",
    "    precision_i = TP / (TP + FP + 1e-8)\n",
    "    f1_i = 2 * recall_i * precision_i / (recall_i + precision_i + 1e-8)\n",
    "\n",
    "    metrics_per_class[\"recall\"].append(recall_i)\n",
    "    metrics_per_class[\"specificity\"].append(specificity_i)\n",
    "    metrics_per_class[\"precision\"].append(precision_i)\n",
    "    metrics_per_class[\"f1\"].append(f1_i)\n",
    "\n",
    "macro_avg_metrics = {k: np.mean(v) for k, v in metrics_per_class.items()}\n",
    "weighted_avg_metrics = {k: np.sum(np.array(v) * weights) for k, v in metrics_per_class.items()}\n",
    "\n",
    "print(\"\\n===== Per-Class Metrics =====\")\n",
    "for k, v in metrics_per_class.items():\n",
    "    print(f\"{k}: {np.round(v, 4)}\")\n",
    "print(\"\\n===== Macro-Average Metrics =====\")\n",
    "for k, v in macro_avg_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "print(\"\\n===== Weighted-Average Metrics =====\")\n",
    "for k, v in weighted_avg_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECG_TF",
   "language": "python",
   "name": "ecg_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
